<!DOCTYPE html>
<html>

<head>
        <title> Talking to Plants: Touché Experiments • Nick Arner </title>
        <meta charset="utf-8">
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="apple-touch-icon" sizes="180x180"  href="/favicon.ico"> 
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico"> 
        <link rel="icon" type="image/png" sizes="16x16"  href="/favicon.ico"> 

        
        <meta property="og:url" content="https://nickarner.com" />
        <meta property="og:title" content="Talking to Plants: Touché Experiments" />
        <meta property="og:description" content="Nick&#39;s new site." />
        <meta property="og:image" content="https://nickarner.com/card.jpg" />

        
        <meta name="twitter:card" content="summary">
        <meta name="twitter:creator" content="@joodaloop">
        <meta name="twitter:title" content="Talking to Plants: Touché Experiments">
        <meta name="twitter:description" content="Nick&#39;s new site." />
        <meta name="twitter:image" content="https://nickarner.com/card.jpg" />

        <meta name="description" content="Nick&#39;s new site.">
        <link rel="stylesheet" type="text/css" href="/css/styles.css" />
</head>

<body>    
    
<main>
        <div class=left>
                <div class=menu>

	<nav>
		<div class="home">
			<a href="/"> Nick Arner </a>
		</div>
		<a href="/investing"> Investments </a>
		<a href="/projects_and_work"> Projects </a>
		<a href="/people"> People </a>
		<a href="/books"> Books </a>
		<a href="/notes"> Notes </a>
	</nav>
</div>
        </div><div class=content>
                <article>

<h1>Talking to Plants: Touché Experiments</h1>
      <div class=date>Published on Aug 4, 2017 </div>
      <p>As I mentioned in a <a href="/notes/tools-for-machine-learning-and-sensor-inputs-for-gesture-recognition-july-6-2017/">previous post</a>, I was really pleased to see that the <a href="https://github.com/damellis/ESP" target="_blank" class="external-link" >ESP-Sensors project</a> had included code for <a href="https://github.com/damellis/ESP/wiki/[Example]-Touch%c3%a9-swept-frequency-capacitive-sensing" target="_blank" class="external-link" >working with a circuit based on Touché</a>.</p>
<p>I had earlier come across other implementations of Touché for the Arduino, but unlike the ESP project, none of them utilized machine learning for classifying gesture types.</p>
<p>Touché is a <a href="https://www.disneyresearch.com/project/touche-touch-and-gesture-sensing-for-the-real-world/" target="_blank" class="external-link" >project developed at Disney Research</a> that uses swept-frequency capacitive sensing to &ldquo;&hellip;not only detect a touch event, but simultaneously recognize complex configurations of the human hands and body during touch interaction.&rdquo;</p>
<p>In other words, it&rsquo;s able to not just tell <em>if</em> a touch event occurred, but what <em>type</em> of touch event occurred. This differs from most capacitive sensors, which are only able to detect whether a touch event occurred, and possibly how far away from a sensor the user&rsquo;s hand is.</p>
<p>Traditional capacitive sensing works by generating an electrical signal at a single frequency. This signal is applied to a conductive surface, such as a metal plate. When a hand is either close to or touching the surface, the value of capacitance changes - signifying that a touch event has occurred, or that a hand is close to the surface of the sensor. The <a href="https://playground.arduino.cc/Main/CapacitiveSensor?from=Main.CapSense" target="_blank" class="external-link" >CapSense library</a> allows for traditional capacitive sensing to be implemented on an Arduino.</p>
<p>Swept-frequency capacitive sensing makes use of <em>multiple</em> frequencies. In their <a href="https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20140805145650/touchechi20121.pdf" target="_blank" class="external-link" >C</a><a href="https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20140805145650/touchechi20121.pdf" target="_blank" class="external-link" >HI 2012 paper</a>, the Touché developers state the reason for using multiple frequencies is &ldquo;Objects excited by an electrical signal respond differently at different frequencies, therefore, the changes in the return signal will also be frequency dependent.&rdquo; Rather than using a single data point generated by an electrical signal at a single frequency, as in traditional capacitive sensing, Touché utilizes multiple data points from multiple generated frequencies.</p>
<p>This capacitive profile is used to train a machine learning pipeline to differentiate between various touch interactions. This machine learning pipeline is based around a Support Vector Machine. Specifically, it uses the <a href="http://www.nickgillian.com/wiki/pmwiki.php?n=GRT.SVM" target="_blank" class="external-link" >SVM module from the Gesture Recognition Toolkit</a>.</p>
<p>Check out the video that accompanied the Touché CHI 2012 paper:</p>
<p><a href="http://www.youtube.com/watch?v=E4tYpXVTjxA" target="_blank" class="external-link" ><img src="http://img.youtube.com/vi/E4tYpXVTjxA/0.jpg" alt="" title=""  />
</a></p>
<p>There have been a few other open-source implementation of touch-sensing based off of Touché that I&rsquo;ve come across before, but the one provided in the ESP project seemed to be the easiest to set-up, and the most usable to work with.</p>
<p>The authors continued their work by showing how Touché could be used to interact with plants in the <a href="https://www.disneyresearch.com/project/botanicus-interacticus-interactive-plant-technology/" target="_blank" class="external-link" >Botanicus Interacticus project</a>, which was displayed in an exhibition at SIGGRAPH 2012:</p>
<p><a href="https://www.youtube.com/watch?v=EcRSKEIucjk" target="_blank" class="external-link" ><img src="http://img.youtube.com/vi/EcRSKEIucjk/0.jpg" alt="" title=""  />
</a></p>
<p>I have two plants on my desk: a fern plant, and an air plant. I really enjoy the way they add some color to my work area, and am grateful for their presence.</p>
<p>I wanted to see if they could talk to me.</p>
<p><img src="/blog_assets/2017/Touche&#43;FernPlantSetup.jpg" alt="Touche&#43;FernPlantSetup" title="Touche&#43;FernPlantSetup"  />
</p>
<p><img src="/blog_assets/2017/Touche&#43;AirPlant-Setup.jpg" alt="Touche&#43;AirPlant-Setup" title="Touche&#43;AirPlant-Setup"  />
</p>
<h2 id="the-fern">The Fern</h2>
<p>I first experimented with the fern. As suggested in the <a href="https://pdfs.semanticscholar.org/bad6/92a87a416a228542f5ed554503b604ad481e.pdf" target="_blank" class="external-link" >Botanicus Interacticus paper</a>, I inserted a simple wire lead into the soil of the plant. This would allow the ESP system to measure the conductive profile of the plant as I touch it.</p>
<p>After doing some experiments, I was able to easily train the ESP system to recognize whether I was touching a single leaf:</p>
<p><img src="/blog_assets/2017/fern.gif" alt="fern" title="fern"  />
</p>
<p>or whether I was lightly caressing down on the top of the leafs with the palm of my hand.</p>
<p><img src="/blog_assets/2017/fern2.gif" alt="fern2" title="fern2"  />
</p>
<p>Here&rsquo;s what that looked like in action:</p>
<p><a href="https://www.youtube.com/watch?v=ZPsU6U54CRM" target="_blank" class="external-link" ><img src="http://img.youtube.com/vi/ZPsU6U54CRM/0.jpg" alt="" title=""  />
</a></p>
<p>I also tried experimenting as to whether or not the system was able to detect whether or not I was touching individual leaves, but was not able to get consistent results. I discuss my theory on why this may be the case at the end of this post.</p>
<h2 id="the-air-plant">The Air Plant</h2>
<p>I experimented with the air plant next, and successfully trained the ESP system to be able to discriminate between whether I had my hand at rest on top of the plant:</p>
<p><img src="/blog_assets/2017/airplant&#43;rest.gif" alt="airplant&#43;rest" title="airplant&#43;rest"  />
</p>
<p>and whether or not was &ldquo;tickling&rdquo; the top of the plant</p>
<p><img src="/blog_assets/2017/ariplant&#43;tickle.gif" alt="ariplant&#43;tickle" title="ariplant&#43;tickle"  />
</p>
<p>Here&rsquo;s what the system looked like in use:</p>
<p><a href="https://www.youtube.com/watch?v=RJ--EB5DpOc" target="_blank" class="external-link" ><img src="http://img.youtube.com/vi/RJ--EB5DpOc/0.jpg" alt="" title=""  />
</a></p>
<p>What I was not successful at was training the ESP to discriminate between the act of touching a leaf, and rubbing my finger on a leaf as shown below:</p>
<p><img src="s/blog_assets/2017/leaf&#43;rub.gif" alt="leaf&#43;rub" title="leaf&#43;rub"  />
</p>
<p>I tried moving the alligator clip from one of the leafs to the root - my theory being that perhaps the capacitance wasn&rsquo;t being spread evenly throughout the plant.</p>
<p><img src="/blog_assets/2017/AirPlant&#43;Electrode.jpg" alt="AirPlant&#43;Electrode" title="AirPlant&#43;Electrode"  />
</p>
<p><img src="/blog_assets/2017/AirPlant&#43;Electrode2.jpg" alt="AirPlant&#43;Electrode2" title="AirPlant&#43;Electrode2"  />
</p>
<p>This appeared to have no affect, however.</p>
<p>I was a bit surprised at this - given the subtlety in touch which it seemed Touché was capable of measuring, I had thought the system would be capable of discriminating between touching and rubbing a single leaf. That said, there could be some missing factor (such as amount of training data/sessions) that I&rsquo;m not aware of yet in order to make that happen.</p>
<h2 id="further-learning-goals">Further Learning Goals:</h2>
<h3 id="using-regression">Using Regression</h3>
<p>In the Bottanicus Interactus video (at the time below), the authors show that they are able to determine where at on a long plant stem is being touched, and interact with it in a way that resembles using a slider moving continuously between two points.</p>
<p><a href="https://www.youtube.com/watch?v=EcRSKEIucjk" target="_blank" class="external-link" ><img src="http://img.youtube.com/vi/EcRSKEIucjk/0.jpg" alt="" title=""  />
</a></p>
<p>The Touché system uses a Support Vector Machine Learning algorithm, which is capable of both classification and regression; two types of machine learning tasks. In classification, a machine learning system detects what <em>type</em> of events have occurred - in this case, the <em>type of touch</em> that occurred. In regression tasks, a machine learning system maps the distance between two points to control a parameter - so, for instance, you could map the distance travelled by the hand between two points on a plant stem to the value of a volume slider.</p>
<p>in the ESP system, classification is currently supported; regression is not. In order to use Touché to control a continuous stream of value between one point and another, the ESP system would need to be modified to support regression.</p>
<p>(For more info on the principles behind classification and regression, check Rebecca Fiebrink&rsquo;s Kadenze course, <a href="https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info" target="_blank" class="external-link" >Machine Learning for Musicians and Artists</a>).</p>
<h3 id="finer-discrimination">Finer Discrimination</h3>
<p>Determine whether or not it is possible to detect the touches of individual leafs, as opposed to detecting whether &ldquo;a leaf&rdquo; has been touched. It may be that this is possible, but dependent on the type of plant involved - a plant with thicker, more &ldquo;solid&rdquo; leaves may return a conductive pattern that&rsquo;s better at discriminating between individual leaf touches than the thin, loose leaves of the fern plant.</p>
<h3 id="gesture-timeouts">Gesture Timeouts</h3>
<p>If you watch the videos of the Touché system in action above, you may have noticed that there were occasionally instances in which there is a short &ldquo;bounce&rdquo; during the transition between one gesture class and another. In the Air Plant example, when the hand is moving from a &ldquo;Resting Hand&rdquo; position to a &ldquo;No Hand&rdquo; position, the ESP system will falsely recognize a &ldquo;Tickling&rdquo; gesture.</p>
<p>A potential remedy for this situation is to add a <a href="http://nickgillian.com/grt/api/0.1.0/_class_label_filter_8h.html" target="_blank" class="external-link" >Class Label Filter</a> to the ESP&rsquo;s gesture recognition pipeline. This class will allow the system to get rid of the erroneously recognized gesture class. Adding this filter to the ESP pipeline is something I&rsquo;m planning on exploring in future experiments.</p>
<h2 id="code">Code</h2>
<p>The Processing sketches shown in the videos, and the ESP session data, can be found <a href="https://github.com/narner/Touche-Experiments" target="_blank" class="external-link" >here</a>. You&rsquo;ll need to make sure you have <a href="http://processing.org/" target="_blank" class="external-link" >Processing</a> installed on your system to run the sketches, and have followed the installation guide for setting the <a href="https://github.com/damellis/ESP" target="_blank" class="external-link" >ESP</a> if you want to run the included sessions.</p>

       

                </article>
                <footer> © Copyright Nicholas Arner, 2022  </footer>
        </div>
        
</main>

        
</body>
</html>